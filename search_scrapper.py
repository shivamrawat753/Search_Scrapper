# -*- coding: utf-8 -*-
"""Search_Scrapper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aStlHfl4cVOjUnry3T9ynGMbiHHt7yBO
"""

import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
import time

def initialize_driver():
    # Set up Chrome options to run the browser in headless mode (without GUI)
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.headless = True
    return webdriver.Chrome(options=chrome_options)

def perform_search(driver, query):
    # Navigate to the base URL, find the search bar, input the query, and submit the search
    driver.get(base_url)
    time.sleep(2)

    search_bar = driver.find_element(By.NAME, "q")
    search_bar.send_keys(query)
    search_bar.send_keys(Keys.RETURN)
    time.sleep(5)

def scroll_page(driver, scrolls=3, sleep_time=2):
    # Scroll down the page to load more search results
    for _ in range(scrolls):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(sleep_time)

def extract_search_results(driver):
    # Extract search results from the page and store them in a list of dictionaries
    search_results = driver.find_elements(By.CSS_SELECTOR, '.MjjYud')
    data_list = []

    for result in search_results:
        try:
            # Extract title, description, and URL from each search result
            title = result.find_element(By.CSS_SELECTOR, 'h3.LC20lb.MBeuO.DKV0Md')
            des = result.find_element(By.CSS_SELECTOR, '.VwiC3b.yXK7lf.lyLwlc.yDYNvb.W8l4ac.lEBKkf')
            link = result.find_element(By.CSS_SELECTOR, 'a').get_attribute("href")
            data = {
                'Title': title.text,
                'Description': des.text,
                'URL': link
            }
            data_list.append(data)

        except Exception as e:
            # Handle exceptions, e.g., if an element is not found
            pass

    return data_list

def save_to_csv(data_list, filename='Internshala_results.csv'):
    # Convert the list of dictionaries into a DataFrame and save it to a CSV file
    df = pd.DataFrame(data_list)
    df.to_csv(filename, index=False)

if __name__ == "__main__":
    # Set the base URL and search query
    base_url = 'https://www.google.com'
    query = 'Internshala'

    # Initialize the Chrome WebDriver
    driver = initialize_driver()

    # Perform the search, scroll the page, extract results, and save to CSV
    perform_search(driver, query)
    scroll_page(driver)
    results_data = extract_search_results(driver)
    save_to_csv(results_data)

    # Quit the WebDriver
    driver.quit()