{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "\n",
        "def initialize_driver():\n",
        "    # Set up Chrome options to run the browser in headless mode (without GUI)\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.headless = True\n",
        "    return webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "def perform_search(driver, query):\n",
        "    # Navigate to the base URL, find the search bar, input the query, and submit the search\n",
        "    driver.get(base_url)\n",
        "    time.sleep(2)\n",
        "\n",
        "    search_bar = driver.find_element(By.NAME, \"q\")\n",
        "    search_bar.send_keys(query)\n",
        "    search_bar.send_keys(Keys.RETURN)\n",
        "    time.sleep(5)\n",
        "\n",
        "def scroll_page(driver, scrolls=3, sleep_time=2):\n",
        "    # Scroll down the page to load more search results\n",
        "    for _ in range(scrolls):\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(sleep_time)\n",
        "\n",
        "def extract_search_results(driver):\n",
        "    # Extract search results from the page and store them in a list of dictionaries\n",
        "    search_results = driver.find_elements(By.CSS_SELECTOR, '.MjjYud')\n",
        "    data_list = []\n",
        "\n",
        "    for result in search_results:\n",
        "        try:\n",
        "            # Extract title, description, and URL from each search result\n",
        "            title = result.find_element(By.CSS_SELECTOR, 'h3.LC20lb.MBeuO.DKV0Md')\n",
        "            des = result.find_element(By.CSS_SELECTOR, '.VwiC3b.yXK7lf.lyLwlc.yDYNvb.W8l4ac.lEBKkf')\n",
        "            link = result.find_element(By.CSS_SELECTOR, 'a').get_attribute(\"href\")\n",
        "            data = {\n",
        "                'Title': title.text,\n",
        "                'Description': des.text,\n",
        "                'URL': link\n",
        "            }\n",
        "            data_list.append(data)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle exceptions, e.g., if an element is not found\n",
        "            pass\n",
        "\n",
        "    return data_list\n",
        "\n",
        "def save_to_csv(data_list, filename='Internshala_results.csv'):\n",
        "    # Convert the list of dictionaries into a DataFrame and save it to a CSV file\n",
        "    df = pd.DataFrame(data_list)\n",
        "    df.to_csv(filename, index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set the base URL and search query\n",
        "    base_url = 'https://www.google.com'\n",
        "    query = 'Internshala'\n",
        "\n",
        "    # Initialize the Chrome WebDriver\n",
        "    driver = initialize_driver()\n",
        "\n",
        "    # Perform the search, scroll the page, extract results, and save to CSV\n",
        "    perform_search(driver, query)\n",
        "    scroll_page(driver)\n",
        "    results_data = extract_search_results(driver)\n",
        "    save_to_csv(results_data)\n",
        "\n",
        "    # Quit the WebDriver\n",
        "    driver.quit()\n",
        ""
      ],
      "metadata": {
        "id": "lLTBR9qWVg0G"
      },
      "execution_count": 43,
      "outputs": []
    }
  ]
}